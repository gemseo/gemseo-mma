{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#gemseo-mma","title":"gemseo-mma","text":""},{"location":"#overview","title":"Overview","text":"<p>A gemseo wrapper of Python version of Method of Moving Asymptothes in the implementation of Arjen Deetman.</p>"},{"location":"#installation","title":"Installation","text":"<p>Install the latest version with <code>pip install gemseo-mma</code>.</p> <p>See pip for more information.</p>"},{"location":"#bugs-and-questions","title":"Bugs and questions","text":"<p>Please use the gitlab issue tracker to submit bugs or questions.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>See the contributing section of GEMSEO.</p>"},{"location":"#contributors","title":"Contributors","text":"<ul> <li>Simone Coniglio</li> <li>Antoine Dechaume</li> <li>Original implementation of Arjen Deetman, see here.</li> </ul>"},{"location":"#references","title":"References","text":"<p>Svanberg, K. (1987). The Method of Moving Asymptotes -- A new method for structural optimization. International Journal for Numerical Methods in Engineering 24, 359-373. <code>doi:10.1002/nme.1620240207</code>, see here.</p> <p>Svanberg, K. (n.d.). MMA and GCMMA -- two methods for nonlinear optimization. Retrieved August 3, 2017 from this.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#changelog","title":"Changelog","text":"<p>All notable changes of this project will be documented here.</p> <p>The format is based on Keep a Changelog and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#version-300-november-2024","title":"Version 3.0.0 (November 2024)","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Support GEMSEO v6.</li> <li>Support for Python 3.12.</li> </ul>"},{"location":"changelog/#version-202-august-2024","title":"Version 2.0.2 (August 2024)","text":""},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>The license of the package is now GPLv3 since it uses a module that is GPLv3.</li> </ul>"},{"location":"changelog/#version-201-december-2023","title":"Version 2.0.1 (December 2023)","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Support for Python 3.11.</li> </ul>"},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>A bug on the option handling was solved for design space normalization   and inequality constraint tolerance.</li> </ul>"},{"location":"changelog/#removed","title":"Removed","text":"<ul> <li>Support for Python 3.8.</li> </ul>"},{"location":"changelog/#version-200-june-2023","title":"Version 2.0.0 (June 2023)","text":"<p>Update to GEMSEO 5.</p>"},{"location":"changelog/#fixed_2","title":"Fixed","text":"<ul> <li>A bug on the option settings was solved.</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>The <code>ctol_abs</code> option was removed, this was anyway not used.</li> <li>The attributes and option names were changed to be more explicit.</li> <li>The solver attributes are made private.</li> </ul>"},{"location":"changelog/#version-100-february-2023","title":"Version 1.0.0 (February 2023)","text":"<p>First release.</p>"},{"location":"credits/","title":"Credits","text":""},{"location":"credits/#exec-1--credits","title":"Credits","text":"<p>The developers thank all the open source libraries making <code>gemseo-mma</code> possible.</p>"},{"location":"credits/#exec-1--external-dependencies","title":"External Dependencies","text":"<p><code>gemseo-mma</code> depends on software with compatible licenses that are listed below.</p> Project License <code>Python</code> Python Software License <code>gemseo</code> GNU Lesser General Public License v3 <code>numpy</code> BSD License <code>scipy</code> BSD License"},{"location":"credits/#exec-1--external-applications","title":"External applications","text":"<p>Some external applications are used by <code>gemseo-mma</code>, but not linked with the application, for testing, documentation generation, training or example purposes.</p> Project License <code>black</code> MIT License <code>commitizen</code> MIT License <code>covdefaults</code> MIT License <code>griffe-inherited-docstrings</code> ISC <code>insert-license</code> MIT <code>markdown-exec</code> ISC <code>mike</code> BSD-3-Clause <code>mkdocs-bibtex</code> BSD-3-Clause-LBNL <code>mkdocs-gallery</code> BSD 3-Clause <code>mkdocs-gen-files</code> MIT License <code>mkdocs-include-markdown-plugin</code> Apache-2.0 <code>mkdocs-literate-nav</code> MIT License <code>mkdocs-material</code> MIT License <code>mkdocs-section-index</code> MIT License <code>mkdocstrings</code> ISC <code>pre-commit</code> MIT License <code>pygrep-hooks</code> MIT <code>pytest</code> MIT License <code>pytest-cov</code> MIT License <code>pytest-xdist</code> MIT License <code>ruff</code> MIT License <code>setuptools</code> MIT License <code>setuptools-scm</code> MIT License"},{"location":"licenses/","title":"Licenses","text":""},{"location":"licenses/#licenses","title":"Licenses","text":""},{"location":"licenses/#gnu-gpl-v30","title":"GNU GPL v3.0","text":"<p>The <code>gemseo-mma</code> source code is distributed under the GNU GPL v3.0 license. <pre><code>Copyright 2021 IRT Saint Exup\u00e9ry, https://www.irt-saintexupery.com\n\nThis program is free software; you can redistribute it and/or\nmodify it under the terms of the GNU General Public\nLicense version 3 as published by the Free Software Foundation.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\nGeneral Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program; if not, write to the Free Software Foundation,\nInc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.\n</code></pre></p>"},{"location":"licenses/#bsd-0-clause","title":"BSD 0-Clause","text":"<p>The <code>gemseo-mma</code> examples are distributed under the BSD 0-Clause <pre><code>Copyright 2021 IRT Saint Exup\u00e9ry, https://www.irt-saintexupery.com\n\nThis work is licensed under a BSD 0-Clause License.\n\nPermission to use, copy, modify, and/or distribute this software\nfor any purpose with or without fee is hereby granted.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\" AND THE AUTHOR DISCLAIMS ALL\nWARRANTIES WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL\nTHE AUTHOR BE LIABLE FOR ANY SPECIAL, DIRECT, INDIRECT,\nOR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING\nFROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT,\nNEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION\nWITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n</code></pre></p>"},{"location":"licenses/#cc-by-sa-40","title":"CC BY-SA 4.0","text":"<p>The <code>gemseo-mma</code> documentation is distributed under the CC BY-SA 4.0 license. <pre><code>Copyright 2021 IRT Saint Exup\u00e9ry, https://www.irt-saintexupery.com\n\nThis work is licensed under the Creative Commons Attribution-ShareAlike 4.0\nInternational License. To view a copy of this license, visit\nhttp://creativecommons.org/licenses/by-sa/4.0/ or send a letter to Creative\nCommons, PO Box 1866, Mountain View, CA 94042, USA.\n</code></pre></p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>gemseo_mma<ul> <li>opt<ul> <li>core<ul> <li>mma</li> <li>mma_optimizer</li> </ul> </li> <li>mma</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/gemseo_mma/","title":"API documentation","text":""},{"location":"reference/gemseo_mma/#gemseo_mma","title":"gemseo_mma","text":"<p>Svanberg MMA optimization solver wrapprer for GEMSEO.</p>"},{"location":"reference/gemseo_mma/opt/","title":"Opt","text":""},{"location":"reference/gemseo_mma/opt/#gemseo_mma.opt","title":"opt","text":"<p>Options for Bi-Level Outer Approximation.</p>"},{"location":"reference/gemseo_mma/opt/mma/","title":"Mma","text":""},{"location":"reference/gemseo_mma/opt/mma/#gemseo_mma.opt.mma","title":"mma","text":"<p>MMA optimizer library.</p>"},{"location":"reference/gemseo_mma/opt/mma/#gemseo_mma.opt.mma-classes","title":"Classes","text":""},{"location":"reference/gemseo_mma/opt/mma/#gemseo_mma.opt.mma.MMASvanberg","title":"MMASvanberg","text":"<p>               Bases: <code>BaseOptimizationLibrary</code></p> <p>Svanberg Method of Moving Asymptotes optimization library.</p> Notes <p>The missing current values of the :class:<code>.DesignSpace</code> attached to the :class:<code>.OptimizationProblem</code> are automatically initialized with the method :meth:<code>.DesignSpace.initialize_missing_current_values</code>.</p>"},{"location":"reference/gemseo_mma/opt/core/","title":"Core","text":""},{"location":"reference/gemseo_mma/opt/core/#gemseo_mma.opt.core","title":"core","text":"<p>Optimization algorithms package.</p>"},{"location":"reference/gemseo_mma/opt/core/mma/","title":"Mma","text":""},{"location":"reference/gemseo_mma/opt/core/mma/#gemseo_mma.opt.core.mma","title":"mma","text":"<p>GCMMA-MMA-Python. This file is part of GCMMA-MMA-Python.</p> <p>GCMMA-MMA-Python is licensed under the terms of GNU General Public License as published by the Free Software Foundation. For more information and the LICENSE file, see here. The original work is written by Krister Svanberg in MATLAB. This is the python version of the code written Arjen Deetman. version 09-11-2019.</p> <p>MMA optimizer.</p> <p>Original work written by Krister Svanberg in Matlab. This is the python version of the code written by Arjen Deetman.</p>"},{"location":"reference/gemseo_mma/opt/core/mma/#gemseo_mma.opt.core.mma-functions","title":"Functions","text":""},{"location":"reference/gemseo_mma/opt/core/mma/#gemseo_mma.opt.core.mma.compute_kkt_residual_on_local_approximation","title":"compute_kkt_residual_on_local_approximation","text":"<pre><code>compute_kkt_residual_on_local_approximation(\n    m: int,\n    n: int,\n    x: ndarray,\n    y: ndarray,\n    z: ndarray,\n    lam: ndarray,\n    xsi: ndarray,\n    eta: ndarray,\n    mu: ndarray,\n    zet: ndarray,\n    s: ndarray,\n    xmin: ndarray,\n    xmax: ndarray,\n    df0dx: ndarray,\n    fval: ndarray,\n    dfdx: ndarray,\n    a0: float,\n    a: ndarray,\n    c: ndarray,\n    d: ndarray,\n) -&gt; tuple[ndarray, ndarray, ndarray]\n</code></pre> <p>KKT residual computation.</p> <p>The left hand sides of the KKT conditions for the following nonlinear programming problem are calculated.</p> <p>Minimize f_0(x) + a_0z + sum(c_iy_i + 0.5d_i(y_i)^2) subject to  f_i(x) - a_i*z - y_i &lt;= 0,   i = 1,...,m             xmax_j &lt;= x_j &lt;= xmin_j,     j = 1,...,n             z &gt;= 0,   y_i &gt;= 0,          i = 1,...,m</p> <p>Parameters:</p> <ul> <li> <code>m</code>               (<code>int</code>)           \u2013            <p>The number of general constraints.</p> </li> <li> <code>n</code>               (<code>int</code>)           \u2013            <p>The number of variables x_j.</p> </li> <li> <code>x</code>               (<code>ndarray</code>)           \u2013            <p>The current values of the n variables x_j.</p> </li> <li> <code>y</code>               (<code>ndarray</code>)           \u2013            <p>The current values of the m variables y_i.</p> </li> <li> <code>z</code>               (<code>ndarray</code>)           \u2013            <p>The current value of the single variable z.</p> </li> <li> <code>lam</code>               (<code>ndarray</code>)           \u2013            <p>The Lagrange multipliers for the m general constraints.</p> </li> <li> <code>xsi</code>               (<code>ndarray</code>)           \u2013            <p>The Lagrange multipliers for the n constraints xmin_j - x_j &lt;= 0.</p> </li> <li> <code>eta</code>               (<code>ndarray</code>)           \u2013            <p>The Lagrange multipliers for the n constraints x_j - xmax_j &lt;= 0.</p> </li> <li> <code>mu</code>               (<code>ndarray</code>)           \u2013            <p>The Lagrange multipliers for the m constraints -y_i &lt;= 0.</p> </li> <li> <code>zet</code>               (<code>ndarray</code>)           \u2013            <p>The Lagrange multiplier for the single constraint -z &lt;= 0.</p> </li> <li> <code>s</code>               (<code>ndarray</code>)           \u2013            <p>The Slack variables for the m general constraints.</p> </li> <li> <code>xmin</code>               (<code>ndarray</code>)           \u2013            <p>The Lower bounds for the variables x_j.</p> </li> <li> <code>xmax</code>               (<code>ndarray</code>)           \u2013            <p>The Upper bounds for the variables x_j.</p> </li> <li> <code>df0dx</code>               (<code>ndarray</code>)           \u2013            <p>The vector with the derivatives of the objective function f_0 with respect to the variables x_j, calculated at x.</p> </li> <li> <code>fval</code>               (<code>ndarray</code>)           \u2013            <p>The vector with the values of the constraint functions f_i, calculated</p> </li> <li> <code>dfdx</code>               (<code>ndarray</code>)           \u2013            <p>The (m x n)-matrix with the derivatives of the constraint functions f_i with respect to the variables x_j, calculated at x. dfdx(i,j) = the derivative of f_i with respect to x_j.</p> </li> <li> <code>a0</code>               (<code>float</code>)           \u2013            <p>The constants a_0 in the term a_0*z.</p> </li> <li> <code>a</code>               (<code>ndarray</code>)           \u2013            <p>The vector with the constants a_i in the terms a_i*z.</p> </li> <li> <code>c</code>               (<code>ndarray</code>)           \u2013            <p>The vector with the constants c_i in the terms c_i*y_i.</p> </li> <li> <code>d</code>               (<code>ndarray</code>)           \u2013            <p>The vector with the constants d_i in the terms 0.5d_i(y_i)^2.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[ndarray, ndarray, ndarray]</code>           \u2013            <p>The vector residual, its norm and its maximum.</p> </li> </ul> Source code in <code>src/gemseo_mma/opt/core/mma.py</code> <pre><code>def compute_kkt_residual_on_local_approximation(\n    m: int,\n    n: int,\n    x: ndarray,\n    y: ndarray,\n    z: ndarray,\n    lam: ndarray,\n    xsi: ndarray,\n    eta: ndarray,\n    mu: ndarray,\n    zet: ndarray,\n    s: ndarray,\n    xmin: ndarray,\n    xmax: ndarray,\n    df0dx: ndarray,\n    fval: ndarray,\n    dfdx: ndarray,\n    a0: float,\n    a: ndarray,\n    c: ndarray,\n    d: ndarray,\n) -&gt; tuple[ndarray, ndarray, ndarray]:\n    \"\"\"KKT residual computation.\n\n    The left hand sides of the KKT conditions for the following nonlinear programming\n    problem are calculated.\n\n    Minimize f_0(x) + a_0*z + sum(c_i*y_i + 0.5*d_i*(y_i)^2)\n    subject to  f_i(x) - a_i*z - y_i &lt;= 0,   i = 1,...,m\n                xmax_j &lt;= x_j &lt;= xmin_j,     j = 1,...,n\n                z &gt;= 0,   y_i &gt;= 0,          i = 1,...,m\n\n    Args:\n        m: The number of general constraints.\n        n: The number of variables x_j.\n        x: The current values of the n variables x_j.\n        y: The current values of the m variables y_i.\n        z: The current value of the single variable z.\n        lam: The Lagrange multipliers for the m general constraints.\n        xsi: The Lagrange multipliers for the n constraints xmin_j - x_j &lt;= 0.\n        eta: The Lagrange multipliers for the n constraints x_j - xmax_j &lt;= 0.\n        mu:  The Lagrange multipliers for the m constraints -y_i &lt;= 0.\n        zet: The Lagrange multiplier for the single constraint -z &lt;= 0.\n        s: The Slack variables for the m general constraints.\n        xmin: The Lower bounds for the variables x_j.\n        xmax: The Upper bounds for the variables x_j.\n        df0dx: The vector with the derivatives of the objective function f_0 with\n            respect to the variables x_j, calculated at x.\n        fval: The vector with the values of the constraint functions f_i, calculated\n        at x.\n        dfdx: The (m x n)-matrix with the derivatives of the constraint functions f_i\n            with respect to the variables x_j, calculated at x.\n            dfdx(i,j) = the derivative of f_i with respect to x_j.\n        a0: The constants a_0 in the term a_0*z.\n        a: The vector with the constants a_i in the terms a_i*z.\n        c: The vector with the constants c_i in the terms c_i*y_i.\n        d: The vector with the constants d_i in the terms 0.5*d_i*(y_i)^2.\n\n    Returns:\n        The vector residual, its norm and its maximum.\n    \"\"\"\n    rex = df0dx + np.dot(dfdx.T, lam) - xsi + eta\n    rey = c + d * y - mu - lam\n    rez = a0 - zet - np.dot(a.T, lam)\n    relam = fval - a * z - y + s\n    rexsi = xsi * (x - xmin)\n    reeta = eta * (xmax - x)\n    remu = mu * y\n    rezet = zet * z\n    res = lam * s\n    residu1 = np.concatenate((rex, rey, rez), axis=0)\n    residu2 = np.concatenate((relam, rexsi, reeta, remu, rezet, res), axis=0)\n    residu = np.concatenate((residu1, residu2), axis=0)\n    residunorm = np.sqrt((np.dot(residu.T, residu)).item())\n    residumax = np.max(np.abs(residu))\n    return residu, residunorm, residumax\n</code></pre>"},{"location":"reference/gemseo_mma/opt/core/mma/#gemseo_mma.opt.core.mma.solve_mma_local_approximation_problem","title":"solve_mma_local_approximation_problem","text":"<pre><code>solve_mma_local_approximation_problem(\n    m: int,\n    n: int,\n    n_iterations: int,\n    xval: ndarray,\n    xmin: ndarray,\n    xmax: ndarray,\n    xold1: ndarray,\n    xold2: ndarray,\n    f0val: ndarray,\n    df0dx: ndarray,\n    fval: ndarray,\n    dfdx: ndarray,\n    low: ndarray,\n    upp: ndarray,\n    a0: float,\n    a: ndarray,\n    c: ndarray,\n    d: ndarray,\n    move: float,\n    external_move_limit: float = 10.0,\n    internal_limit: float = 0.01,\n    asyinit: float = 0.5,\n    asyincr: float = 1.2,\n    asydecr: float = 0.7,\n) -&gt; tuple[\n    ndarray,\n    ndarray,\n    ndarray,\n    ndarray,\n    ndarray,\n    ndarray,\n    ndarray,\n    ndarray,\n    ndarray,\n    ndarray,\n    ndarray,\n]\n</code></pre> <p>MMA sub function.</p> <p>This function mmasub performs one MMA-iteration, aimed at solving the nonlinear programming problem:</p> <p>Minimize    f_0(x) + a_0z + sum( c_iy_i + 0.5d_i(y_i)^2 ) subject to  f_i(x) - a_i*z - y_i &lt;= 0,  i = 1,...,m             xmin_j &lt;= x_j &lt;= xmax_j,    j = 1,...,n             z &gt;= 0,   y_i &gt;= 0,         i = 1,...,m</p> <p>Parameters:</p> <ul> <li> <code>m</code>               (<code>int</code>)           \u2013            <p>The number of general constraints.</p> </li> <li> <code>n</code>               (<code>int</code>)           \u2013            <p>The number of variables x_j.</p> </li> <li> <code>n_iterations</code>               (<code>int</code>)           \u2013            <p>The current iteration number (=1 the first time mmasub is called).</p> </li> <li> <code>xval</code>               (<code>ndarray</code>)           \u2013            <p>The column vector with the current values of the variables x_j.</p> </li> <li> <code>xmin</code>               (<code>ndarray</code>)           \u2013            <p>The column vector with the lower bounds for the variables x_j.</p> </li> <li> <code>xmax</code>               (<code>ndarray</code>)           \u2013            <p>The column vector with the upper bounds for the variables x_j.</p> </li> <li> <code>xold1</code>               (<code>ndarray</code>)           \u2013            <p>The value of xval, one iteration ago (provided that n_iterations&gt;1).</p> </li> <li> <code>xold2</code>               (<code>ndarray</code>)           \u2013            <p>The value of xval, two iterations ago (provided that n_iterations&gt;2).</p> </li> <li> <code>f0val</code>               (<code>ndarray</code>)           \u2013            <p>The value of the objective function f_0 at xval.</p> </li> <li> <code>df0dx</code>               (<code>ndarray</code>)           \u2013            <p>The column vector with the derivatives of the objective function f_0 with respect to the variables x_j, calculated at xval.</p> </li> <li> <code>fval</code>               (<code>ndarray</code>)           \u2013            <p>The column vector with the values of the constraint functions f_i, calculated at xval.</p> </li> <li> <code>dfdx</code>               (<code>ndarray</code>)           \u2013            <p>The (m x n)-matrix with the derivatives of the constraint functions f_i with respect to the variables x_j, calculated at xval.</p> </li> <li> <code>low</code>               (<code>ndarray</code>)           \u2013            <p>The column vector with the lower asymptotes from the previous iteration (provided that n_iterations&gt;1).</p> </li> <li> <code>upp</code>               (<code>ndarray</code>)           \u2013            <p>The column vector with the upper asymptotes from the previous iteration (provided that n_iterations&gt;1).</p> </li> <li> <code>a0</code>               (<code>float</code>)           \u2013            <p>The constants a_0 in the term a_0*z.</p> </li> <li> <code>a</code>               (<code>ndarray</code>)           \u2013            <p>The column vector with the constants a_i in the terms a_i*z.</p> </li> <li> <code>c</code>               (<code>ndarray</code>)           \u2013            <p>The column vector with the constants c_i in the terms c_i*y_i.</p> </li> <li> <code>d</code>               (<code>ndarray</code>)           \u2013            <p>The column vector with the constants d_i in the terms 0.5d_i(y_i)^2.</p> </li> <li> <code>move</code>               (<code>float</code>)           \u2013            <p>The maximum optimization step.</p> </li> <li> <code>external_move_limit</code>               (<code>float</code>, default:                   <code>10.0</code> )           \u2013            <p>The maximum distance of the asymptotes from the current design variable value.</p> </li> <li> <code>internal_limit</code>               (<code>float</code>, default:                   <code>0.01</code> )           \u2013            <p>The minimum distance of the asymptotes from the current design variable value.</p> </li> <li> <code>asyinit</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The initial asymptotes distance from the current design variable value.</p> </li> <li> <code>asyincr</code>               (<code>float</code>, default:                   <code>1.2</code> )           \u2013            <p>The incremental factor for successful iterations.</p> </li> <li> <code>asydecr</code>               (<code>float</code>, default:                   <code>0.7</code> )           \u2013            <p>The decremental factor for unsuccessful iterations.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray]</code>           \u2013            <p>The Column vector with the optimal values of the variables x_j in the current MMA subproblem.</p> <p>The Column vector with the optimal values of the variables y_i in the current MMA subproblem.</p> <p>The Scalar with the optimal value of the variable z in the current MMA subproblem.</p> <p>The Lagrange multipliers for the m general MMA constraints.</p> <p>The Lagrange multipliers for the n constraints alfa_j - x_j &lt;= 0.</p> <p>The Lagrange multipliers for the n constraints x_j - beta_j &lt;= 0.</p> <p>The Lagrange multipliers for the m constraints -y_i &lt;= 0.</p> <p>The Lagrange multiplier for the single constraint -z &lt;= 0.</p> <p>The Slack variables for the m general MMA constraints.</p> <p>The Column vector with the lower asymptotes, calculated and used in the current MMA subproblem.</p> <p>The Column vector with the upper asymptotes, calculated and used in the current MMA subproblem.</p> </li> </ul> Source code in <code>src/gemseo_mma/opt/core/mma.py</code> <pre><code>def solve_mma_local_approximation_problem(\n    m: int,\n    n: int,\n    n_iterations: int,\n    xval: ndarray,\n    xmin: ndarray,\n    xmax: ndarray,\n    xold1: ndarray,\n    xold2: ndarray,\n    f0val: ndarray,\n    df0dx: ndarray,\n    fval: ndarray,\n    dfdx: ndarray,\n    low: ndarray,\n    upp: ndarray,\n    a0: float,\n    a: ndarray,\n    c: ndarray,\n    d: ndarray,\n    move: float,\n    external_move_limit: float = 10.0,\n    internal_limit: float = 0.01,\n    asyinit: float = 0.5,\n    asyincr: float = 1.2,\n    asydecr: float = 0.7,\n) -&gt; tuple[\n    ndarray,\n    ndarray,\n    ndarray,\n    ndarray,\n    ndarray,\n    ndarray,\n    ndarray,\n    ndarray,\n    ndarray,\n    ndarray,\n    ndarray,\n]:\n    \"\"\"MMA sub function.\n\n    This function mmasub performs one MMA-iteration, aimed at solving the nonlinear\n    programming problem:\n\n    Minimize    f_0(x) + a_0*z + sum( c_i*y_i + 0.5*d_i*(y_i)^2 )\n    subject to  f_i(x) - a_i*z - y_i &lt;= 0,  i = 1,...,m\n                xmin_j &lt;= x_j &lt;= xmax_j,    j = 1,...,n\n                z &gt;= 0,   y_i &gt;= 0,         i = 1,...,m\n\n    Args:\n        m: The number of general constraints.\n        n: The number of variables x_j.\n        n_iterations: The current iteration number\n            (=1 the first time mmasub is called).\n        xval: The column vector with the current values of the variables x_j.\n        xmin: The column vector with the lower bounds for the variables x_j.\n        xmax: The column vector with the upper bounds for the variables x_j.\n        xold1: The value of xval, one iteration ago (provided that n_iterations&gt;1).\n        xold2: The value of xval, two iterations ago (provided that n_iterations&gt;2).\n        f0val: The value of the objective function f_0 at xval.\n        df0dx: The column vector with the derivatives of the objective function\n            f_0 with respect to the variables x_j, calculated at xval.\n        fval: The column vector with the values of the constraint functions f_i,\n            calculated at xval.\n        dfdx: The (m x n)-matrix with the derivatives of the constraint functions\n            f_i with respect to the variables x_j, calculated at xval.\n        low: The column vector with the lower asymptotes from the previous\n            iteration (provided that n_iterations&gt;1).\n        upp: The column vector with the upper asymptotes from the previous\n            iteration (provided that n_iterations&gt;1).\n        a0: The constants a_0 in the term a_0*z.\n        a: The column vector with the constants a_i in the terms a_i*z.\n        c: The column vector with the constants c_i in the terms c_i*y_i.\n        d: The column vector with the constants d_i in the terms 0.5*d_i*(y_i)^2.\n        move: The maximum optimization step.\n        external_move_limit: The maximum distance of the asymptotes from the current\n            design variable value.\n        internal_limit: The minimum distance of the asymptotes from the current design\n            variable value.\n        asyinit: The initial asymptotes distance from the current design variable value.\n        asyincr: The incremental factor for successful iterations.\n        asydecr: The decremental factor for unsuccessful iterations.\n\n    Returns:\n        The Column vector with the optimal values of the variables x_j\n        in the current MMA subproblem.\n\n        The Column vector with the optimal values of the variables y_i\n        in the current MMA subproblem.\n\n        The Scalar with the optimal value of the variable z\n        in the current MMA subproblem.\n\n        The Lagrange multipliers for the m general MMA constraints.\n\n        The Lagrange multipliers for the n constraints alfa_j - x_j &lt;= 0.\n\n        The Lagrange multipliers for the n constraints x_j - beta_j &lt;= 0.\n\n        The Lagrange multipliers for the m constraints -y_i &lt;= 0.\n\n        The Lagrange multiplier for the single constraint -z &lt;= 0.\n\n        The Slack variables for the m general MMA constraints.\n\n        The Column vector with the lower asymptotes, calculated and used\n        in the current MMA subproblem.\n\n        The Column vector with the upper asymptotes, calculated and used\n        in the current MMA subproblem.\n    \"\"\"\n    epsimin = 0.0000001\n    raa0 = 0.00001\n    albefa = 0.1\n    eeen = np.ones((n, 1))\n    eeem = np.ones((m, 1))\n    zeron = np.zeros((n, 1))\n    # Calculation of the asymptotes low and upp\n    if n_iterations &lt;= 2:\n        low = xval - asyinit * (xmax - xmin)\n        upp = xval + asyinit * (xmax - xmin)\n    else:\n        zzz = (xval - xold1) * (xold1 - xold2)\n        factor = eeen.copy()\n        factor[np.where(zzz &gt; 0)] = asyincr\n        factor[np.where(zzz &lt; 0)] = asydecr\n        low = xval - factor * (xold1 - low)\n        upp = xval + factor * (upp - xold1)\n        lowmin = xval - external_move_limit * (xmax - xmin)\n        lowmax = xval - internal_limit * (xmax - xmin)\n        uppmin = xval + internal_limit * (xmax - xmin)\n        uppmax = xval + external_move_limit * (xmax - xmin)\n        low = np.maximum(low, lowmin)\n        low = np.minimum(low, lowmax)\n        upp = np.minimum(upp, uppmax)\n        upp = np.maximum(upp, uppmin)\n    # Calculation of the bounds alfa and beta\n    zzz1 = low + albefa * (xval - low)\n    zzz2 = xval - move * (xmax - xmin)\n    zzz = np.maximum(zzz1, zzz2)\n    alfa = np.maximum(zzz, xmin)\n    zzz1 = upp - albefa * (upp - xval)\n    zzz2 = xval + move * (xmax - xmin)\n    zzz = np.minimum(zzz1, zzz2)\n    beta = np.minimum(zzz, xmax)\n    # Calculations of p0, q0, pp, qq and b\n    xmami = xmax - xmin\n    xmamieps = 0.00001 * eeen\n    xmami = np.maximum(xmami, xmamieps)\n    xmamiinv = eeen / xmami\n    ux1 = upp - xval\n    ux2 = ux1 * ux1\n    xl1 = xval - low\n    xl2 = xl1 * xl1\n    uxinv = eeen / ux1\n    xlinv = eeen / xl1\n    p0 = zeron.copy()\n    q0 = zeron.copy()\n    p0 = np.maximum(df0dx, 0)\n    q0 = np.maximum(-df0dx, 0)\n    pq0 = 0.001 * (p0 + q0) + raa0 * xmamiinv\n    p0 = p0 + pq0\n    q0 = q0 + pq0\n    p0 = p0 * ux2\n    q0 = q0 * xl2\n    pp = np.zeros((m, n))\n    qq = np.zeros((m, n))\n    pp = np.maximum(dfdx, 0)\n    qq = np.maximum(-dfdx, 0)\n    ppqq = 0.001 * (pp + qq) + raa0 * np.dot(eeem, xmamiinv.T)\n    pp = pp + ppqq\n    qq = qq + ppqq\n    pp = (diags(ux2.flatten(), 0).dot(pp.T)).T\n    qq = (diags(xl2.flatten(), 0).dot(qq.T)).T\n    b = np.dot(pp, uxinv) + np.dot(qq, xlinv) - fval\n    xmma, ymma, zmma, lam, xsi, eta, mu, zet, s = __subsolv(\n        m, n, epsimin, low, upp, alfa, beta, p0, q0, pp, qq, a0, a, b, c, d\n    )\n    # Return values\n    return xmma, ymma, zmma, lam, xsi, eta, mu, zet, s, low, upp\n</code></pre>"},{"location":"reference/gemseo_mma/opt/core/mma_optimizer/","title":"Mma optimizer","text":""},{"location":"reference/gemseo_mma/opt/core/mma_optimizer/#gemseo_mma.opt.core.mma_optimizer","title":"mma_optimizer","text":"<p>MMA optimization solver.</p>"},{"location":"reference/gemseo_mma/opt/core/mma_optimizer/#gemseo_mma.opt.core.mma_optimizer-classes","title":"Classes","text":""},{"location":"reference/gemseo_mma/opt/core/mma_optimizer/#gemseo_mma.opt.core.mma_optimizer.MMAOptimizer","title":"MMAOptimizer","text":"<pre><code>MMAOptimizer(problem: OptimizationProblem)\n</code></pre> <p>Method of Moving Asymptotes optimizer class.</p> <p>This class run an optimization algorithm to solve Non-linear Optimization problems with constraints. The objective function and the constraints and their gradients are needed for the optimization algorithm. The original implementation the next iteration candidate is computed using mmasub function adapted from this. The external and internal move limit can be tuned to control minimum and maximum local approximation convexity. The max_optimization_step parameter can be used to control the optimization step. To avoid solver divergence in the case of highly non-linear problems one should use smaller values of the <code>max_optimization_step</code>, <code>max_asymptote_distance</code> and <code>min_asymptote_distance</code>.</p> <p>Constructor.</p> Source code in <code>src/gemseo_mma/opt/core/mma_optimizer.py</code> <pre><code>def __init__(self, problem: OptimizationProblem) -&gt; None:\n    \"\"\"Constructor.\"\"\"\n    self.__problem = problem\n    self.__message = self.__EMPTY_STRING\n    self.__normalize_design_space = self.__DEFAULT_NORMALIZE_DESIGN_SPACE\n    self.__max_iter = self.__DEFAULT_MAXITER\n    self.__tol = self.__DEFAULT_KKT_TOL\n    self.__xtol_abs = self.__DEFAULT_TOLERANCE\n    self.__xtol_rel = self.__DEFAULT_TOLERANCE\n    self.__ftol_rel = self.__DEFAULT_TOLERANCE\n    self.__ftol_abs = self.__DEFAULT_TOLERANCE\n    self.__ineq_tolerance = self.__DEFAULT_TOLERANCE\n    self.__max_asymptote_distance = self.__DEFAULT_MAX_DISTANCE\n    self.__min_asymptote_distance = self.__DEFAULT_MIN_DISTANCE\n    self.__max_optimization_step = self.__DEFAULT_MAX_OPTIM_STEP\n    self.__initial_asymptotes_distance = self.__DEFAULT_ASYINIT\n    self.__asymptotes_distance_amplification_coefficient = self.__DEFAULT_ASYINCR\n    self.__asymptotes_distance_reduction_coefficient = self.__DEFAULT_ASYDECR\n</code></pre>"},{"location":"reference/gemseo_mma/opt/core/mma_optimizer/#gemseo_mma.opt.core.mma_optimizer.MMAOptimizer-functions","title":"Functions","text":""},{"location":"reference/gemseo_mma/opt/core/mma_optimizer/#gemseo_mma.opt.core.mma_optimizer.MMAOptimizer.iterate","title":"iterate","text":"<pre><code>iterate(x0: ndarray) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Iterate until convergence from the starting guess.</p> <p>Parameters:</p> <ul> <li> <code>x0</code>               (<code>ndarray</code>)           \u2013            <p>The starting guess design point.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[ndarray, ndarray]</code>           \u2013            <p>The optimum design point and objective value.</p> </li> </ul> Source code in <code>src/gemseo_mma/opt/core/mma_optimizer.py</code> <pre><code>def iterate(self, x0: ndarray) -&gt; tuple[ndarray, ndarray]:\n    \"\"\"Iterate until convergence from the starting guess.\n\n    Args:\n        x0: The starting guess design point.\n\n    Returns:\n        The optimum design point and objective value.\n    \"\"\"\n    n = len(x0)\n    eeen = np.ones((n, 1))\n    xval = np.reshape(x0, eeen.shape)\n    xold1 = xval.copy()\n    xold2 = xval.copy()\n    if not self.__normalize_design_space:\n        xmin = np.reshape(\n            self.__problem.design_space.get_lower_bounds(), eeen.shape\n        )\n        xmax = np.reshape(\n            self.__problem.design_space.get_upper_bounds(), eeen.shape\n        )\n    else:\n        xmin = 0 * eeen\n        xmax = eeen\n    low = xmin.copy()\n    upp = xmax.copy()\n\n    a0 = 1\n\n    outeriter = 0\n    maxoutit = self.__max_iter\n    kkttol = self.__tol\n    (\n        f0val,\n        df0dx,\n        fval,\n        dfdx,\n    ) = self.__get_objective_and_constraints_with_gradients(xval.flatten())\n    m = len(fval)\n    eeem = np.ones((m, 1))\n    zerom = np.zeros((m, 1))\n    c = 1000 * eeem\n    d = eeem.copy()\n    a = zerom.copy()\n    # The iterations starts\n    kktnorm = kkttol + 10\n    f_ref = f0val\n    x_ref = np.linalg.norm(xval)\n    change_fc = 10\n    change_f = 10\n    change_x = 10\n    change_relative_f = 10\n    change_relative_x = 10\n    outit = 0\n    while (\n        (\n            (kktnorm &gt; kkttol)\n            and (change_x &gt; self.__xtol_abs)\n            and (change_relative_x &gt; self.__xtol_rel)\n            and (change_relative_f &gt; self.__ftol_rel)\n            and (change_f &gt; self.__ftol_abs)\n        )\n        or (any(fval &gt; self.__ineq_tolerance) and change_fc &gt; self.__ftol_abs)\n    ) and (outit &lt; maxoutit):\n        outit += 1\n        outeriter += 1\n\n        (\n            xmma,\n            ymma,\n            zmma,\n            lam,\n            xsi,\n            eta,\n            mu,\n            zet,\n            s,\n            low,\n            upp,\n        ) = solve_mma_local_approximation_problem(\n            m,\n            n,\n            outeriter,\n            xval,\n            xmin,\n            xmax,\n            xold1,\n            xold2,\n            f0val,\n            df0dx,\n            fval,\n            dfdx,\n            low,\n            upp,\n            a0,\n            a,\n            c,\n            d,\n            self.__max_optimization_step,\n            self.__max_asymptote_distance,\n            self.__min_asymptote_distance,\n            self.__initial_asymptotes_distance,\n            self.__asymptotes_distance_amplification_coefficient,\n            self.__asymptotes_distance_reduction_coefficient,\n        )\n        (\n            f0valnew,\n            df0dxnew,\n            fvalnew,\n            dfdxnew,\n        ) = self.__get_objective_and_constraints_with_gradients(xmma.flatten())\n        change_x = np.linalg.norm(xval - xmma)\n        change_f = np.abs(f0valnew - f0val)\n        change_fc = np.abs(fvalnew.max() - fval.max())\n        change_relative_x = change_x / x_ref\n        change_relative_f = change_f / f_ref\n        # Some vectors are updated:\n        xold2 = xold1.copy()\n        xold1 = xval.copy()\n        xval = xmma.copy()\n        # Re-calculate function values and gradients of the objective and\n        # constraints functions\n        f0val, df0dx, fval, dfdx = f0valnew, df0dxnew, fvalnew, dfdxnew\n        # The residual vector of the KKT conditions is calculated\n        _, kktnorm, _ = compute_kkt_residual_on_local_approximation(\n            m,\n            n,\n            xmma,\n            ymma,\n            zmma,\n            lam,\n            xsi,\n            eta,\n            mu,\n            zet,\n            s,\n            xmin,\n            xmax,\n            df0dx,\n            fval,\n            dfdx,\n            a0,\n            a,\n            c,\n            d,\n        )\n\n    if self.__normalize_design_space:\n        xopt = self.__problem.design_space.unnormalize_vect(xval.flatten())\n    else:\n        xopt = xval.flatten()\n\n    LOGGER.info(\"END OF MMA ALGORITHM\")\n    if kktnorm &lt;= kkttol:\n        self.__message = \"KKT norm criteria met\"\n    else:\n        self.__message = \"change criteria met\"\n    return xopt, f0val\n</code></pre>"},{"location":"reference/gemseo_mma/opt/core/mma_optimizer/#gemseo_mma.opt.core.mma_optimizer.MMAOptimizer.optimize","title":"optimize","text":"<pre><code>optimize(**options: bool | float) -&gt; tuple[str, int]\n</code></pre> <p>Optimize the problem.</p> <p>Parameters:</p> <ul> <li> <code>**options</code>               (<code>bool | float</code>, default:                   <code>{}</code> )           \u2013            <p>The optimization problem options.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[str, int]</code>           \u2013            <p>The optimization solver message and final status.</p> </li> </ul> Source code in <code>src/gemseo_mma/opt/core/mma_optimizer.py</code> <pre><code>def optimize(self, **options: bool | float) -&gt; tuple[str, int]:\n    \"\"\"Optimize the problem.\n\n    Args:\n        **options: The optimization problem options.\n\n    Returns:\n        The optimization solver message and final status.\n    \"\"\"\n    self.__normalize_design_space = options.get(\n        \"normalize_design_space\", self.__DEFAULT_NORMALIZE_DESIGN_SPACE\n    )\n    self.__max_iter = options.get(\"max_iter\", self.__DEFAULT_MAXITER)\n    self.__tol = options.get(\"tol\", self.__DEFAULT_TOLERANCE)\n    self.__xtol_abs = options.get(\"xtol_abs\", self.__DEFAULT_TOLERANCE)\n    self.__xtol_rel = options.get(\"xtol_rel\", self.__DEFAULT_TOLERANCE)\n    self.__ftol_rel = options.get(\"ftol_rel\", self.__DEFAULT_TOLERANCE)\n    self.__ftol_abs = options.get(\"ftol_abs\", self.__DEFAULT_TOLERANCE)\n    self.__max_asymptote_distance = options.get(\n        \"max_asymptote_distance\", self.__DEFAULT_MAX_DISTANCE\n    )\n    self.__min_asymptote_distance = options.get(\n        \"min_asymptote_distance\", self.__DEFAULT_MIN_DISTANCE\n    )\n    self.__max_optimization_step = options.get(\n        \"max_optimization_step\", self.__DEFAULT_MAX_OPTIM_STEP\n    )\n    self.__initial_asymptotes_distance = options.get(\n        \"initial_asymptotes_distance\", self.__DEFAULT_ASYINIT\n    )\n    self.__asymptotes_distance_amplification_coefficient = options.get(\n        \"asymptotes_distance_amplification_coefficient\", self.__DEFAULT_ASYINCR\n    )\n    self.__asymptotes_distance_reduction_coefficient = options.get(\n        \"asymptotes_distance_reduction_coefficient\", self.__DEFAULT_ASYDECR\n    )\n    self.__ineq_tolerance = options.get(\"ineq_tolerance\", self.__DEFAULT_TOLERANCE)\n\n    # initialize database\n    if not self.__normalize_design_space:\n        x0 = self.__problem.design_space.get_current_value()\n    else:\n        x0 = self.__problem.design_space.normalize_vect(\n            self.__problem.design_space.get_current_value()\n        )\n\n    # launch optim\n    xopt = self.iterate(x0)[0]\n    self.__problem.design_space.set_current_value(xopt)\n\n    return self.__message, 0\n</code></pre>"},{"location":"reference/gemseo_mma/opt/core/mma_optimizer/#gemseo_mma.opt.core.mma_optimizer-functions","title":"Functions","text":""},{"location":"user_guide/","title":"User guide","text":""},{"location":"user_guide/#user-guide","title":"User guide","text":"<p>Like any other gemseo wrapped solver, MMA solver can be called setting the algo option to <code>\"MMA\"</code>. This algorithm can be used for single objective continuous optimization problem with non-linear inequality constraints.</p> <p>Advanced options:</p> <ul> <li><code>tol</code>: The KKT residual norm tolerance. This is not the one     implemented in GEMSEO as it uses the local functions to be computed.</li> <li><code>max_optimization_step</code>: Also known as <code>move</code> parameter control the     maximum distance of the next iteration design point from the current     one. Reducing this parameter avoid divergence for highly non-linear     problems.</li> <li><code>min_asymptote_distance</code>: The minimum distance of the asymptotes from     the current design variable value.</li> <li><code>max_asymptote_distance</code>: The maximum distance of the asymptotes from     the current design variable value.</li> <li><code>initial_asymptotes_distance</code>: The initial asymptote distance from the     current design variable value.</li> <li><code>asymptotes_distance_amplification_coefficient</code> The incremental factor     of asymptote distance from the current design variable value for     successful iterations.</li> <li><code>asymptotes_distance_reduction_coefficient</code>: The decremental factor of     asymptote distance from the current design variable value for     successful iterations.</li> <li><code>conv_tol</code>: If provided control all other convergence tolerances.</li> </ul> <p>The shortest is the distance of the asymptotes, the highest is the convexity of the local approximation. It's another mechanism to control the optimization step. You can find this example.</p>"}]}